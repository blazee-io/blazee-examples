{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Starting training...\n",
      "[1]\tvalid_0's l2: 0.242763\tvalid_0's l1: 0.491735\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's l2: 0.237895\tvalid_0's l1: 0.486563\n",
      "[3]\tvalid_0's l2: 0.233277\tvalid_0's l1: 0.481489\n",
      "[4]\tvalid_0's l2: 0.22925\tvalid_0's l1: 0.476848\n",
      "[5]\tvalid_0's l2: 0.226155\tvalid_0's l1: 0.47305\n",
      "[6]\tvalid_0's l2: 0.222963\tvalid_0's l1: 0.469049\n",
      "[7]\tvalid_0's l2: 0.220364\tvalid_0's l1: 0.465556\n",
      "[8]\tvalid_0's l2: 0.217872\tvalid_0's l1: 0.462208\n",
      "[9]\tvalid_0's l2: 0.215328\tvalid_0's l1: 0.458676\n",
      "[10]\tvalid_0's l2: 0.212743\tvalid_0's l1: 0.454998\n",
      "[11]\tvalid_0's l2: 0.210805\tvalid_0's l1: 0.452047\n",
      "[12]\tvalid_0's l2: 0.208945\tvalid_0's l1: 0.449158\n",
      "[13]\tvalid_0's l2: 0.206986\tvalid_0's l1: 0.44608\n",
      "[14]\tvalid_0's l2: 0.205513\tvalid_0's l1: 0.443554\n",
      "[15]\tvalid_0's l2: 0.203728\tvalid_0's l1: 0.440643\n",
      "[16]\tvalid_0's l2: 0.201865\tvalid_0's l1: 0.437687\n",
      "[17]\tvalid_0's l2: 0.200639\tvalid_0's l1: 0.435454\n",
      "[18]\tvalid_0's l2: 0.199522\tvalid_0's l1: 0.433288\n",
      "[19]\tvalid_0's l2: 0.198552\tvalid_0's l1: 0.431297\n",
      "[20]\tvalid_0's l2: 0.197238\tvalid_0's l1: 0.428946\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l2: 0.197238\tvalid_0's l1: 0.428946\n",
      "Starting predicting...\n",
      "The rmse of prediction is: 0.4441153344254208\n",
      "Feature importances: [23, 7, 0, 33, 5, 56, 9, 1, 1, 21, 2, 5, 1, 19, 9, 6, 1, 10, 4, 10, 0, 31, 61, 4, 48, 102, 52, 79]\n",
      "Starting training with custom eval function...\n",
      "[1]\tvalid_0's l2: 0.242763\tvalid_0's RMSLE: 0.344957\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's l2: 0.237895\tvalid_0's RMSLE: 0.341693\n",
      "[3]\tvalid_0's l2: 0.233277\tvalid_0's RMSLE: 0.338462\n",
      "[4]\tvalid_0's l2: 0.22925\tvalid_0's RMSLE: 0.335656\n",
      "[5]\tvalid_0's l2: 0.226155\tvalid_0's RMSLE: 0.333431\n",
      "[6]\tvalid_0's l2: 0.222963\tvalid_0's RMSLE: 0.331104\n",
      "[7]\tvalid_0's l2: 0.220364\tvalid_0's RMSLE: 0.329193\n",
      "[8]\tvalid_0's l2: 0.217872\tvalid_0's RMSLE: 0.327337\n",
      "[9]\tvalid_0's l2: 0.215328\tvalid_0's RMSLE: 0.325433\n",
      "[10]\tvalid_0's l2: 0.212743\tvalid_0's RMSLE: 0.323523\n",
      "[11]\tvalid_0's l2: 0.210805\tvalid_0's RMSLE: 0.321986\n",
      "[12]\tvalid_0's l2: 0.208945\tvalid_0's RMSLE: 0.320523\n",
      "[13]\tvalid_0's l2: 0.206986\tvalid_0's RMSLE: 0.319027\n",
      "[14]\tvalid_0's l2: 0.205513\tvalid_0's RMSLE: 0.317796\n",
      "[15]\tvalid_0's l2: 0.203728\tvalid_0's RMSLE: 0.316383\n",
      "[16]\tvalid_0's l2: 0.201865\tvalid_0's RMSLE: 0.314827\n",
      "[17]\tvalid_0's l2: 0.200639\tvalid_0's RMSLE: 0.313876\n",
      "[18]\tvalid_0's l2: 0.199522\tvalid_0's RMSLE: 0.312948\n",
      "[19]\tvalid_0's l2: 0.198552\tvalid_0's RMSLE: 0.312116\n",
      "[20]\tvalid_0's l2: 0.197238\tvalid_0's RMSLE: 0.311032\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l2: 0.197238\tvalid_0's RMSLE: 0.311032\n",
      "Starting training with multiple custom eval functions...\n",
      "[1]\tvalid_0's l2: 0.242763\tvalid_0's RMSLE: 0.344957\tvalid_0's RAE: 0.991146\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's l2: 0.237895\tvalid_0's RMSLE: 0.341693\tvalid_0's RAE: 0.98072\n",
      "[3]\tvalid_0's l2: 0.233277\tvalid_0's RMSLE: 0.338462\tvalid_0's RAE: 0.970493\n",
      "[4]\tvalid_0's l2: 0.22925\tvalid_0's RMSLE: 0.335656\tvalid_0's RAE: 0.961139\n",
      "[5]\tvalid_0's l2: 0.226155\tvalid_0's RMSLE: 0.333431\tvalid_0's RAE: 0.953484\n",
      "[6]\tvalid_0's l2: 0.222963\tvalid_0's RMSLE: 0.331104\tvalid_0's RAE: 0.945419\n",
      "[7]\tvalid_0's l2: 0.220364\tvalid_0's RMSLE: 0.329193\tvalid_0's RAE: 0.938379\n",
      "[8]\tvalid_0's l2: 0.217872\tvalid_0's RMSLE: 0.327337\tvalid_0's RAE: 0.931631\n",
      "[9]\tvalid_0's l2: 0.215328\tvalid_0's RMSLE: 0.325433\tvalid_0's RAE: 0.92451\n",
      "[10]\tvalid_0's l2: 0.212743\tvalid_0's RMSLE: 0.323523\tvalid_0's RAE: 0.917099\n",
      "[11]\tvalid_0's l2: 0.210805\tvalid_0's RMSLE: 0.321986\tvalid_0's RAE: 0.911151\n",
      "[12]\tvalid_0's l2: 0.208945\tvalid_0's RMSLE: 0.320523\tvalid_0's RAE: 0.905328\n",
      "[13]\tvalid_0's l2: 0.206986\tvalid_0's RMSLE: 0.319027\tvalid_0's RAE: 0.899122\n",
      "[14]\tvalid_0's l2: 0.205513\tvalid_0's RMSLE: 0.317796\tvalid_0's RAE: 0.894031\n",
      "[15]\tvalid_0's l2: 0.203728\tvalid_0's RMSLE: 0.316383\tvalid_0's RAE: 0.888164\n",
      "[16]\tvalid_0's l2: 0.201865\tvalid_0's RMSLE: 0.314827\tvalid_0's RAE: 0.882206\n",
      "[17]\tvalid_0's l2: 0.200639\tvalid_0's RMSLE: 0.313876\tvalid_0's RAE: 0.877704\n",
      "[18]\tvalid_0's l2: 0.199522\tvalid_0's RMSLE: 0.312948\tvalid_0's RAE: 0.87334\n",
      "[19]\tvalid_0's l2: 0.198552\tvalid_0's RMSLE: 0.312116\tvalid_0's RAE: 0.869327\n",
      "[20]\tvalid_0's l2: 0.197238\tvalid_0's RMSLE: 0.311032\tvalid_0's RAE: 0.864588\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l2: 0.197238\tvalid_0's RMSLE: 0.311032\tvalid_0's RAE: 0.864588\n",
      "Starting predicting...\n",
      "The rmsle of prediction is: 0.3110323289863277\n",
      "The rae of prediction is: 0.8645881044669875\n",
      "Best parameters found by grid search are: {'learning_rate': 0.1, 'n_estimators': 40}\n"
     ]
    }
   ],
   "source": [
    "# Taken from https://github.com/Microsoft/LightGBM/blob/master/examples/python-guide/simple_example.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print('Loading data...')\n",
    "# load or create your dataset\n",
    "df_train = pd.read_csv('./regression.train.txt', header=None, sep='\\t')\n",
    "df_test = pd.read_csv('./regression.test.txt', header=None, sep='\\t')\n",
    "\n",
    "y_train = df_train[0]\n",
    "y_test = df_test[0]\n",
    "X_train = df_train.drop(0, axis=1)\n",
    "X_test = df_test.drop(0, axis=1)\n",
    "\n",
    "print('Starting training...')\n",
    "# train\n",
    "gbm = lgb.LGBMRegressor(num_leaves=31,\n",
    "                        learning_rate=0.05,\n",
    "                        n_estimators=20)\n",
    "gbm.fit(X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_metric='l1',\n",
    "        early_stopping_rounds=5)\n",
    "\n",
    "print('Starting predicting...')\n",
    "# predict\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\n",
    "# eval\n",
    "print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)\n",
    "\n",
    "# feature importances\n",
    "print('Feature importances:', list(gbm.feature_importances_))\n",
    "\n",
    "\n",
    "# self-defined eval metric\n",
    "# f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool\n",
    "# Root Mean Squared Logarithmic Error (RMSLE)\n",
    "def rmsle(y_true, y_pred):\n",
    "    return 'RMSLE', np.sqrt(np.mean(np.power(np.log1p(y_pred) - np.log1p(y_true), 2))), False\n",
    "\n",
    "\n",
    "print('Starting training with custom eval function...')\n",
    "# train\n",
    "gbm.fit(X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_metric=rmsle,\n",
    "        early_stopping_rounds=5)\n",
    "\n",
    "\n",
    "# another self-defined eval metric\n",
    "# f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool\n",
    "# Relative Absolute Error (RAE)\n",
    "def rae(y_true, y_pred):\n",
    "    return 'RAE', np.sum(np.abs(y_pred - y_true)) / np.sum(np.abs(np.mean(y_true) - y_true)), False\n",
    "\n",
    "\n",
    "print('Starting training with multiple custom eval functions...')\n",
    "# train\n",
    "gbm.fit(X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_metric=lambda y_true, y_pred: [rmsle(y_true, y_pred), rae(y_true, y_pred)],\n",
    "        early_stopping_rounds=5)\n",
    "\n",
    "print('Starting predicting...')\n",
    "# predict\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\n",
    "# eval\n",
    "print('The rmsle of prediction is:', rmsle(y_test, y_pred)[1])\n",
    "print('The rae of prediction is:', rae(y_test, y_pred)[1])\n",
    "\n",
    "# other scikit-learn modules\n",
    "estimator = lgb.LGBMRegressor(num_leaves=31)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'n_estimators': [20, 40]\n",
    "}\n",
    "\n",
    "gbm = GridSearchCV(estimator, param_grid, cv=3)\n",
    "gbm.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters found by grid search are:', gbm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Uploading model version to Blazee  (108.8 KB)...\n",
      "INFO:root:Deploying new model version: v1...\n",
      "INFO:root:Successfully deployed model version 0ecf3524-a003-4367-be6b-79ce3856d989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BlazeeModel 'SK LightGBM Regressor'\n",
       "\tid=2adba5ba-9a0c-4571-9072-9ba0f18468da>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import blazee\n",
    "import os\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Deploy the model on Blazee\n",
    "api_key = os.environ['BLAZEE_API_KEY']\n",
    "bz = blazee.Blazee(api_key)\n",
    "bm = bz.deploy_model(gbm, model_name=\"SK LightGBM Regressor\")\n",
    "bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8257646282148502,\n",
       " 0.4123071018043463,\n",
       " 0.24269368102594482,\n",
       " 0.4696884764490895,\n",
       " 0.24659772613405778,\n",
       " 0.33670517896588925,\n",
       " 0.36510372822253734,\n",
       " 0.384108996126978,\n",
       " 0.7135510119323984,\n",
       " 0.36821617725769756,\n",
       " 0.618979758082155,\n",
       " 0.7787357351263059,\n",
       " 0.7583265448995183,\n",
       " 0.7469671278799553,\n",
       " 0.46549842187233004,\n",
       " 0.766383794393904,\n",
       " 0.39381455084446637,\n",
       " 0.6029948474423068,\n",
       " 0.6134054497397179,\n",
       " 0.5399952990183657,\n",
       " 0.7392494515151498,\n",
       " 0.6492501428744724,\n",
       " 0.5316622297336435,\n",
       " 0.4655239343539504,\n",
       " 0.5003303077752469,\n",
       " 0.6289086395189045,\n",
       " 0.5961926540349629,\n",
       " 0.8153805638279291,\n",
       " 0.4501762904530452,\n",
       " 0.7867929020048664,\n",
       " 0.6289618620986811,\n",
       " 0.6653706406929076,\n",
       " 0.33620955250842777,\n",
       " 0.6021165275273311,\n",
       " 0.4827693556053621,\n",
       " 0.28082298844689957,\n",
       " 0.2672589691845787,\n",
       " 0.10695389007437166,\n",
       " 0.5296159758279994,\n",
       " 0.8082771894726081,\n",
       " 0.08489917657112316,\n",
       " 0.704823865967765,\n",
       " 0.3063660357841089,\n",
       " 0.4148757439305074,\n",
       " 0.2866171410703307,\n",
       " -0.04961621997673224,\n",
       " 0.5774784170012971,\n",
       " 0.4707576442650812,\n",
       " 0.8974019415583712,\n",
       " 0.479683335946174,\n",
       " 1.0233996620236645,\n",
       " 0.3944737311365597,\n",
       " 0.13866534256050286,\n",
       " 0.8374728032338167,\n",
       " 0.7892325789227956,\n",
       " 0.4999321660947926,\n",
       " 0.1462938194454814,\n",
       " 0.38747213625713933,\n",
       " 0.6192920192904298,\n",
       " 0.7952365354281515,\n",
       " 0.3745659675545526,\n",
       " 0.8389429545524684,\n",
       " 0.8192489771412163,\n",
       " 0.32747004032245414,\n",
       " 0.7807894137466391,\n",
       " 0.2839385458445461,\n",
       " 0.5205355724173283,\n",
       " 0.15417987215465107,\n",
       " 0.8733930318738927,\n",
       " 0.8051356451257876,\n",
       " 0.3462716523519074,\n",
       " 0.7825404038263791,\n",
       " 0.9022078917491761,\n",
       " 0.23481000579440903,\n",
       " 0.937230200039867,\n",
       " 0.9345758697163139,\n",
       " 0.35654914536544646,\n",
       " 0.4989175847507018,\n",
       " 0.7878941397439565,\n",
       " 0.6217975347582502,\n",
       " 0.243282449948565,\n",
       " 0.5803326491092824,\n",
       " 0.8064070550157285,\n",
       " 1.0211524860120844,\n",
       " 0.1059423154703715,\n",
       " 0.7652146539607955,\n",
       " 0.554921140958732,\n",
       " 0.5499350712358596,\n",
       " 0.43301099512391733,\n",
       " 0.5691994234050751,\n",
       " 0.3185196712899574,\n",
       " 0.3371119991801193,\n",
       " 0.3475278244623506,\n",
       " 0.598977629743227,\n",
       " 0.5369284899765325,\n",
       " 0.8375178133268271,\n",
       " 0.2064332240343172,\n",
       " 0.313786544125231,\n",
       " 0.8803277627076128,\n",
       " 0.2096778203367958,\n",
       " 0.3458570573303426,\n",
       " 0.39895777026746715,\n",
       " 0.6383558964353153,\n",
       " 0.2000623417413307,\n",
       " 0.41274161858054614,\n",
       " 0.8908524774181971,\n",
       " 0.6851719305152189,\n",
       " 0.6193762560293825,\n",
       " 0.18446613976748866,\n",
       " 0.2747223389261196,\n",
       " 0.38275820472547983,\n",
       " 0.6721136193826405,\n",
       " 0.39411544668328957,\n",
       " 0.02987837816019711,\n",
       " 0.6305215421349214,\n",
       " 0.9787307776075849,\n",
       " 0.2417681260831433,\n",
       " 0.8444497326749156,\n",
       " 0.19491852671931034,\n",
       " 0.5070091481630572,\n",
       " 0.28783058443958565,\n",
       " 0.7979238601962265,\n",
       " 0.7695049322262189,\n",
       " 0.6807246904459817,\n",
       " 0.2433928098532593,\n",
       " 0.5872003915278983,\n",
       " 0.7112252195676331,\n",
       " 0.49660920051508556,\n",
       " 0.3013447945066095,\n",
       " 0.6878224574592224,\n",
       " 0.699638463085814,\n",
       " 0.5025274761026886,\n",
       " 0.24923712275527277,\n",
       " 0.30353986076790956,\n",
       " 0.5695862442397386,\n",
       " 0.8561678758164045,\n",
       " 0.3826290759276913,\n",
       " 0.8378826557408554,\n",
       " 0.42269544440155415,\n",
       " 0.5943350717349605,\n",
       " 0.31589399927682854,\n",
       " 0.5469625696126421,\n",
       " 0.6065789173616196,\n",
       " 0.5059351702573573,\n",
       " 0.4633461672049075,\n",
       " 0.16610133901988625,\n",
       " 0.21777446015021362,\n",
       " 0.61195543839627,\n",
       " 0.8173693115295227,\n",
       " 0.22846147855631313,\n",
       " 0.3868448781015061,\n",
       " 0.5512950656037545,\n",
       " 0.7792598497158421,\n",
       " 0.3425232476700168,\n",
       " 0.7487660736827518,\n",
       " 0.5294412707672073,\n",
       " 0.5912717046636569,\n",
       " 0.4981178896456482,\n",
       " 0.5632392030117109,\n",
       " 0.32210330768420425,\n",
       " 0.915440270703651,\n",
       " 0.8616438862479479,\n",
       " 0.7184530964172512,\n",
       " 0.06999155362211787,\n",
       " 0.7035259040556984,\n",
       " 0.3508158033440396,\n",
       " 0.6072542470367871,\n",
       " 0.868100768516379,\n",
       " 0.24425465124868548,\n",
       " 0.9001263939802834,\n",
       " 0.8933433465296527,\n",
       " 0.30862973563261237,\n",
       " 0.3399345611626198,\n",
       " 0.793508929599628,\n",
       " 0.38353123367817843,\n",
       " 0.6962409508448602,\n",
       " 0.6810498942074014,\n",
       " 0.4913373398273891,\n",
       " 0.24255959100781488,\n",
       " 0.054894809838916085,\n",
       " 0.14875698075616733,\n",
       " 0.4690965799512441,\n",
       " 0.17912544688154933,\n",
       " 0.2817515187736067,\n",
       " 0.13275371740032502,\n",
       " 0.8869334288871151,\n",
       " 0.67110704115863,\n",
       " 0.5253733850372474,\n",
       " 0.9116229320758583,\n",
       " 0.3600657726428506,\n",
       " 0.6390829587423041,\n",
       " 0.042642491510384836,\n",
       " 0.539355890969689,\n",
       " 0.3093642566891009,\n",
       " 0.38640908962599296,\n",
       " 0.702428517615783,\n",
       " 0.549013212161642,\n",
       " 0.3557299813673614,\n",
       " 0.12946088253390875,\n",
       " 0.6140440867898144,\n",
       " 0.32407939272138436,\n",
       " 0.1869088013372633,\n",
       " 0.638452069927318,\n",
       " 0.1974849679308231,\n",
       " 0.835808493688266,\n",
       " 0.2527256325934189,\n",
       " 0.7373569835767255,\n",
       " 0.39672535028322764,\n",
       " 0.8358031960189569,\n",
       " 0.1902473715819393,\n",
       " 0.32444686963320324,\n",
       " 0.8004329818449746,\n",
       " 0.45869983964016037,\n",
       " 0.3517207171874283,\n",
       " 0.4312418769736655,\n",
       " 0.334666378405346,\n",
       " 0.8124149539139756,\n",
       " 0.35488788048541753,\n",
       " 0.43346722092545054,\n",
       " 0.5499325268732599,\n",
       " 0.7549457128362853,\n",
       " 0.10592254315675348,\n",
       " 0.7294316393673158,\n",
       " 0.6188411528305992,\n",
       " 0.3816986502178484,\n",
       " 0.9618041591912542,\n",
       " 0.410114848961434,\n",
       " 0.4310962163185851,\n",
       " 0.6336557645155407,\n",
       " 0.20168448948412582,\n",
       " 0.6669344437373843,\n",
       " 0.2908410502846478,\n",
       " 0.541684679713165,\n",
       " 0.5795210015661935,\n",
       " 0.4107828386261992,\n",
       " 0.2572969452595688,\n",
       " 0.5814997130432792,\n",
       " 0.3352462270019226,\n",
       " 0.695743581989988,\n",
       " 0.5554176854183825,\n",
       " 0.6213643772581647,\n",
       " 0.15139050870161727,\n",
       " 0.08533661654062924,\n",
       " 0.3714863966196099,\n",
       " 0.45050701382828146,\n",
       " 0.2715335518031208,\n",
       " 0.1660973695937625,\n",
       " 0.6724751505477672,\n",
       " 0.5273217964870983,\n",
       " 0.11279383824031415,\n",
       " 0.2611599696092812,\n",
       " 0.30489320033311207,\n",
       " 0.34728283832885914,\n",
       " 0.8528855511328726,\n",
       " 0.6774578237969733,\n",
       " 0.5828523234704212,\n",
       " 0.6155306651544485,\n",
       " 0.8505751383020542,\n",
       " 0.25758504615675804,\n",
       " 0.26846965903890774,\n",
       " 0.420215303344646,\n",
       " 0.4321797601479751,\n",
       " 0.30994203016519745,\n",
       " 0.631728720590178,\n",
       " 0.33804148577726,\n",
       " 0.743662832354801,\n",
       " 0.7320346973487558,\n",
       " 0.29320917476194847,\n",
       " 0.5826611586197195,\n",
       " 0.047708295893505885,\n",
       " 0.38489819963343197,\n",
       " 0.4626456701990721,\n",
       " 0.364544114710536,\n",
       " 0.5893277124256969,\n",
       " 0.7796165049443124,\n",
       " 0.6329445337973985,\n",
       " 0.5334673496888788,\n",
       " 0.854146268777646,\n",
       " 0.5003148567644324,\n",
       " 0.3436423219119986,\n",
       " 0.48852371769654973,\n",
       " 0.2632799999792615,\n",
       " 0.34435190427611095,\n",
       " 0.4832251318933028,\n",
       " 0.4014048105759052,\n",
       " 0.2178647307218257,\n",
       " 0.3156474021859439,\n",
       " 0.24598190463724265,\n",
       " 0.28265770009877456,\n",
       " 0.6702255692442844,\n",
       " 0.7624735160096597,\n",
       " 0.11804514115276167,\n",
       " 0.30334487468487487,\n",
       " 0.8090669558587053,\n",
       " 0.2739205061392871,\n",
       " 0.3594416626116594,\n",
       " 0.2701423710238194,\n",
       " 0.2670925954851027,\n",
       " 0.39393686030078956,\n",
       " 0.48740010316258775,\n",
       " 0.3240940045468883,\n",
       " 0.37001841742668495,\n",
       " 0.4193838707341332,\n",
       " 0.40083176836803797,\n",
       " 0.27773042994774605,\n",
       " 0.9304240845770917,\n",
       " 0.3192589355520363,\n",
       " 0.727555857437437,\n",
       " 0.6407766886708107,\n",
       " 0.5415552493529455,\n",
       " 0.8440667943332641,\n",
       " 0.4989753281592895,\n",
       " 0.3347080837547017,\n",
       " 0.895679157842456,\n",
       " 0.39433974821312734,\n",
       " 0.7454568025782315,\n",
       " 0.5608190975825986,\n",
       " 0.646773069568975,\n",
       " 0.5918601141834955,\n",
       " 0.21371800184018483,\n",
       " 0.5270788516373651,\n",
       " 0.6217964443585988,\n",
       " 0.38361004152715755,\n",
       " 0.834245779060345,\n",
       " 0.8741022158346077,\n",
       " 0.22286403972044408,\n",
       " 0.2707991993929498,\n",
       " 0.515015968563432,\n",
       " 0.33456758261166114,\n",
       " 0.5501798097787237,\n",
       " 0.3919624659603717,\n",
       " 0.6728036109584841,\n",
       " 0.6389339536157755,\n",
       " 0.41260921567981246,\n",
       " 0.10383942014421398,\n",
       " 0.2897346026336292,\n",
       " 0.5281682051096639,\n",
       " 0.3757075765289549,\n",
       " 0.9941441057729298,\n",
       " 0.33494884580432904,\n",
       " 0.6948423288638326,\n",
       " 0.40100972431387744,\n",
       " 0.5533664112797024,\n",
       " 0.30822103525706923,\n",
       " 0.8092778797123584,\n",
       " 0.49406398367597965,\n",
       " 0.6226050271540415,\n",
       " 0.8812474579077408,\n",
       " 0.36817737930413824,\n",
       " 0.6487999254396658,\n",
       " 0.5287848966948324,\n",
       " 0.8739498384607497,\n",
       " 0.7248699772234245,\n",
       " 0.23025885944410523,\n",
       " 0.44114422279573423,\n",
       " 0.31590466725884137,\n",
       " 0.5273494451484425,\n",
       " 0.27204489753206246,\n",
       " 0.9089358649839667,\n",
       " 0.2920664425400596,\n",
       " 0.3515408233249818,\n",
       " 0.329299428078692,\n",
       " 0.607392900235054,\n",
       " 0.2755791639021486,\n",
       " 0.2932375357237813,\n",
       " 0.7211922838894153,\n",
       " 0.4153812343245712,\n",
       " 0.27415743478897214,\n",
       " 0.5120836259366203,\n",
       " 0.8526211609950195,\n",
       " 0.4201578160746368,\n",
       " 0.060569024959028155,\n",
       " 0.5901622375246593,\n",
       " 0.7650367875205287,\n",
       " 0.8236922913586189,\n",
       " 0.7129771084526897,\n",
       " 0.5962362724710424,\n",
       " 0.5654842045541858,\n",
       " 0.6893171489776736,\n",
       " 0.14840030927657585,\n",
       " 0.47935809629498904,\n",
       " 0.921551036648585,\n",
       " 0.8372005386249162,\n",
       " 0.32934135261607966,\n",
       " 0.8725402521826131,\n",
       " 0.5798361407220125,\n",
       " 0.5769732328003223,\n",
       " 0.6452465277568059,\n",
       " 0.2335225202400873,\n",
       " 0.38133657106925456,\n",
       " 0.5880877039998925,\n",
       " 0.3112769933218271,\n",
       " 0.7172535082472867,\n",
       " 0.48565329693806186,\n",
       " 0.8385654277802779,\n",
       " 0.33771233068071793,\n",
       " 0.6699790860809954,\n",
       " 0.7182412257181473,\n",
       " 0.7165279594568255,\n",
       " 0.1858317004938739,\n",
       " 0.617601213970478,\n",
       " 0.5717739301422838,\n",
       " 0.6271327181248969,\n",
       " 0.7347315718607886,\n",
       " 0.6863514799974278,\n",
       " 0.851892307302846,\n",
       " 0.5822592244149224,\n",
       " 0.8598312353622684,\n",
       " 0.8057889367691563,\n",
       " 0.3060778091635415,\n",
       " 0.8316825831019188,\n",
       " 0.48581173206371014,\n",
       " 0.3846042544987798,\n",
       " 0.8674422090357903,\n",
       " 0.4328337748077813,\n",
       " 0.5719306747732179,\n",
       " 0.9790861875816825,\n",
       " 0.5874292587527316,\n",
       " 0.507260253549313,\n",
       " 0.37473146317755957,\n",
       " 0.8080013073396853,\n",
       " 0.2823430516433758,\n",
       " 0.40478822831445577,\n",
       " 0.847709828194163,\n",
       " 0.24717436406107934,\n",
       " 0.6812898302254801,\n",
       " 0.46974613135373466,\n",
       " 0.5328075802517015,\n",
       " 0.12797302001518918,\n",
       " 0.4103780301675989,\n",
       " 0.29000679655600614,\n",
       " 0.5463006893197374,\n",
       " 0.7970892586488979,\n",
       " 0.8909709385010309,\n",
       " 0.37365106100557177,\n",
       " 0.7741788439957188,\n",
       " 0.4347977357534891,\n",
       " 0.565766028166895,\n",
       " 0.5094214874459266,\n",
       " 0.9579914254261542,\n",
       " 0.2850967163166084,\n",
       " 0.500484722384551,\n",
       " 0.29493476216296216,\n",
       " 0.5878723980896886,\n",
       " 0.9249563027492238,\n",
       " 0.8018544936943033,\n",
       " 0.4923851598817274,\n",
       " 0.823909119733053,\n",
       " 0.4122215305766067,\n",
       " 0.6345120630238879,\n",
       " 0.05246561314159816,\n",
       " 0.4704787400487897,\n",
       " 0.46664371069546867,\n",
       " 0.5135390293711031,\n",
       " 0.5012633371470963,\n",
       " 0.34134923164498604,\n",
       " 0.7871363028477522,\n",
       " 0.4841947569774625,\n",
       " 0.32158652961047285,\n",
       " 0.4946692890414126,\n",
       " 0.609865198308717,\n",
       " 0.5353091306892619,\n",
       " 0.2947429985343287,\n",
       " 0.5952574210899428,\n",
       " 0.09821725666174476,\n",
       " 0.7442412850364767,\n",
       " 0.6031565037853233,\n",
       " 0.5629147733111954,\n",
       " 0.42442221334386476,\n",
       " 0.4280877652542642,\n",
       " 0.22129666883624,\n",
       " 0.4122408706424028,\n",
       " 0.6889595003881669,\n",
       " 0.5229743897750172,\n",
       " 0.48053862884627835,\n",
       " 0.495686904272049,\n",
       " 0.5841168630820519,\n",
       " 0.5696220371062871,\n",
       " 0.40833229172395535,\n",
       " 0.5178398178708096,\n",
       " 0.6160938650724035,\n",
       " 0.8287054121943899,\n",
       " 0.5324077553555476,\n",
       " 0.7748302649140852,\n",
       " 0.5526289969637379,\n",
       " 0.6193221863517716,\n",
       " 0.5011144432169313,\n",
       " 0.6746714597036818,\n",
       " 0.7106417868279783,\n",
       " 0.5972429771841915,\n",
       " 0.6979753119315913,\n",
       " 0.4179419308655427,\n",
       " 0.44187199837233043,\n",
       " 0.44389134030214855,\n",
       " 0.9094441435193018,\n",
       " 0.826807087928681,\n",
       " 0.3373553806863556,\n",
       " 0.7286017546392779,\n",
       " 0.285297187700545,\n",
       " 0.34869136398490785]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions from Blazee model\n",
    "[p.prediction for p in bm.predict_batch(X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.82576463,  0.4123071 ,  0.24269368,  0.46968848,  0.24659773,\n",
       "        0.33670518,  0.36510373,  0.384109  ,  0.71355101,  0.36821618,\n",
       "        0.61897976,  0.77873574,  0.75832654,  0.74696713,  0.46549842,\n",
       "        0.76638379,  0.39381455,  0.60299485,  0.61340545,  0.5399953 ,\n",
       "        0.73924945,  0.64925014,  0.53166223,  0.46552393,  0.50033031,\n",
       "        0.62890864,  0.59619265,  0.81538056,  0.45017629,  0.7867929 ,\n",
       "        0.62896186,  0.66537064,  0.33620955,  0.60211653,  0.48276936,\n",
       "        0.28082299,  0.26725897,  0.10695389,  0.52961598,  0.80827719,\n",
       "        0.08489918,  0.70482387,  0.30636604,  0.41487574,  0.28661714,\n",
       "       -0.04961622,  0.57747842,  0.47075764,  0.89740194,  0.47968334,\n",
       "        1.02339966,  0.39447373,  0.13866534,  0.8374728 ,  0.78923258,\n",
       "        0.49993217,  0.14629382,  0.38747214,  0.61929202,  0.79523654,\n",
       "        0.37456597,  0.83894295,  0.81924898,  0.32747004,  0.78078941,\n",
       "        0.28393855,  0.52053557,  0.15417987,  0.87339303,  0.80513565,\n",
       "        0.34627165,  0.7825404 ,  0.90220789,  0.23481001,  0.9372302 ,\n",
       "        0.93457587,  0.35654915,  0.49891758,  0.78789414,  0.62179753,\n",
       "        0.24328245,  0.58033265,  0.80640706,  1.02115249,  0.10594232,\n",
       "        0.76521465,  0.55492114,  0.54993507,  0.433011  ,  0.56919942,\n",
       "        0.31851967,  0.337112  ,  0.34752782,  0.59897763,  0.53692849,\n",
       "        0.83751781,  0.20643322,  0.31378654,  0.88032776,  0.20967782,\n",
       "        0.34585706,  0.39895777,  0.6383559 ,  0.20006234,  0.41274162,\n",
       "        0.89085248,  0.68517193,  0.61937626,  0.18446614,  0.27472234,\n",
       "        0.3827582 ,  0.67211362,  0.39411545,  0.02987838,  0.63052154,\n",
       "        0.97873078,  0.24176813,  0.84444973,  0.19491853,  0.50700915,\n",
       "        0.28783058,  0.79792386,  0.76950493,  0.68072469,  0.24339281,\n",
       "        0.58720039,  0.71122522,  0.4966092 ,  0.30134479,  0.68782246,\n",
       "        0.69963846,  0.50252748,  0.24923712,  0.30353986,  0.56958624,\n",
       "        0.85616788,  0.38262908,  0.83788266,  0.42269544,  0.59433507,\n",
       "        0.315894  ,  0.54696257,  0.60657892,  0.50593517,  0.46334617,\n",
       "        0.16610134,  0.21777446,  0.61195544,  0.81736931,  0.22846148,\n",
       "        0.38684488,  0.55129507,  0.77925985,  0.34252325,  0.74876607,\n",
       "        0.52944127,  0.5912717 ,  0.49811789,  0.5632392 ,  0.32210331,\n",
       "        0.91544027,  0.86164389,  0.7184531 ,  0.06999155,  0.7035259 ,\n",
       "        0.3508158 ,  0.60725425,  0.86810077,  0.24425465,  0.90012639,\n",
       "        0.89334335,  0.30862974,  0.33993456,  0.79350893,  0.38353123,\n",
       "        0.69624095,  0.68104989,  0.49133734,  0.24255959,  0.05489481,\n",
       "        0.14875698,  0.46909658,  0.17912545,  0.28175152,  0.13275372,\n",
       "        0.88693343,  0.67110704,  0.52537339,  0.91162293,  0.36006577,\n",
       "        0.63908296,  0.04264249,  0.53935589,  0.30936426,  0.38640909,\n",
       "        0.70242852,  0.54901321,  0.35572998,  0.12946088,  0.61404409,\n",
       "        0.32407939,  0.1869088 ,  0.63845207,  0.19748497,  0.83580849,\n",
       "        0.25272563,  0.73735698,  0.39672535,  0.8358032 ,  0.19024737,\n",
       "        0.32444687,  0.80043298,  0.45869984,  0.35172072,  0.43124188,\n",
       "        0.33466638,  0.81241495,  0.35488788,  0.43346722,  0.54993253,\n",
       "        0.75494571,  0.10592254,  0.72943164,  0.61884115,  0.38169865,\n",
       "        0.96180416,  0.41011485,  0.43109622,  0.63365576,  0.20168449,\n",
       "        0.66693444,  0.29084105,  0.54168468,  0.579521  ,  0.41078284,\n",
       "        0.25729695,  0.58149971,  0.33524623,  0.69574358,  0.55541769,\n",
       "        0.62136438,  0.15139051,  0.08533662,  0.3714864 ,  0.45050701,\n",
       "        0.27153355,  0.16609737,  0.67247515,  0.5273218 ,  0.11279384,\n",
       "        0.26115997,  0.3048932 ,  0.34728284,  0.85288555,  0.67745782,\n",
       "        0.58285232,  0.61553067,  0.85057514,  0.25758505,  0.26846966,\n",
       "        0.4202153 ,  0.43217976,  0.30994203,  0.63172872,  0.33804149,\n",
       "        0.74366283,  0.7320347 ,  0.29320917,  0.58266116,  0.0477083 ,\n",
       "        0.3848982 ,  0.46264567,  0.36454411,  0.58932771,  0.7796165 ,\n",
       "        0.63294453,  0.53346735,  0.85414627,  0.50031486,  0.34364232,\n",
       "        0.48852372,  0.26328   ,  0.3443519 ,  0.48322513,  0.40140481,\n",
       "        0.21786473,  0.3156474 ,  0.2459819 ,  0.2826577 ,  0.67022557,\n",
       "        0.76247352,  0.11804514,  0.30334487,  0.80906696,  0.27392051,\n",
       "        0.35944166,  0.27014237,  0.2670926 ,  0.39393686,  0.4874001 ,\n",
       "        0.324094  ,  0.37001842,  0.41938387,  0.40083177,  0.27773043,\n",
       "        0.93042408,  0.31925894,  0.72755586,  0.64077669,  0.54155525,\n",
       "        0.84406679,  0.49897533,  0.33470808,  0.89567916,  0.39433975,\n",
       "        0.7454568 ,  0.5608191 ,  0.64677307,  0.59186011,  0.213718  ,\n",
       "        0.52707885,  0.62179644,  0.38361004,  0.83424578,  0.87410222,\n",
       "        0.22286404,  0.2707992 ,  0.51501597,  0.33456758,  0.55017981,\n",
       "        0.39196247,  0.67280361,  0.63893395,  0.41260922,  0.10383942,\n",
       "        0.2897346 ,  0.52816821,  0.37570758,  0.99414411,  0.33494885,\n",
       "        0.69484233,  0.40100972,  0.55336641,  0.30822104,  0.80927788,\n",
       "        0.49406398,  0.62260503,  0.88124746,  0.36817738,  0.64879993,\n",
       "        0.5287849 ,  0.87394984,  0.72486998,  0.23025886,  0.44114422,\n",
       "        0.31590467,  0.52734945,  0.2720449 ,  0.90893586,  0.29206644,\n",
       "        0.35154082,  0.32929943,  0.6073929 ,  0.27557916,  0.29323754,\n",
       "        0.72119228,  0.41538123,  0.27415743,  0.51208363,  0.85262116,\n",
       "        0.42015782,  0.06056902,  0.59016224,  0.76503679,  0.82369229,\n",
       "        0.71297711,  0.59623627,  0.5654842 ,  0.68931715,  0.14840031,\n",
       "        0.4793581 ,  0.92155104,  0.83720054,  0.32934135,  0.87254025,\n",
       "        0.57983614,  0.57697323,  0.64524653,  0.23352252,  0.38133657,\n",
       "        0.5880877 ,  0.31127699,  0.71725351,  0.4856533 ,  0.83856543,\n",
       "        0.33771233,  0.66997909,  0.71824123,  0.71652796,  0.1858317 ,\n",
       "        0.61760121,  0.57177393,  0.62713272,  0.73473157,  0.68635148,\n",
       "        0.85189231,  0.58225922,  0.85983124,  0.80578894,  0.30607781,\n",
       "        0.83168258,  0.48581173,  0.38460425,  0.86744221,  0.43283377,\n",
       "        0.57193067,  0.97908619,  0.58742926,  0.50726025,  0.37473146,\n",
       "        0.80800131,  0.28234305,  0.40478823,  0.84770983,  0.24717436,\n",
       "        0.68128983,  0.46974613,  0.53280758,  0.12797302,  0.41037803,\n",
       "        0.2900068 ,  0.54630069,  0.79708926,  0.89097094,  0.37365106,\n",
       "        0.77417884,  0.43479774,  0.56576603,  0.50942149,  0.95799143,\n",
       "        0.28509672,  0.50048472,  0.29493476,  0.5878724 ,  0.9249563 ,\n",
       "        0.80185449,  0.49238516,  0.82390912,  0.41222153,  0.63451206,\n",
       "        0.05246561,  0.47047874,  0.46664371,  0.51353903,  0.50126334,\n",
       "        0.34134923,  0.7871363 ,  0.48419476,  0.32158653,  0.49466929,\n",
       "        0.6098652 ,  0.53530913,  0.294743  ,  0.59525742,  0.09821726,\n",
       "        0.74424129,  0.6031565 ,  0.56291477,  0.42442221,  0.42808777,\n",
       "        0.22129667,  0.41224087,  0.6889595 ,  0.52297439,  0.48053863,\n",
       "        0.4956869 ,  0.58411686,  0.56962204,  0.40833229,  0.51783982,\n",
       "        0.61609387,  0.82870541,  0.53240776,  0.77483026,  0.552629  ,\n",
       "        0.61932219,  0.50111444,  0.67467146,  0.71064179,  0.59724298,\n",
       "        0.69797531,  0.41794193,  0.441872  ,  0.44389134,  0.90944414,\n",
       "        0.82680709,  0.33735538,  0.72860175,  0.28529719,  0.34869136])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
